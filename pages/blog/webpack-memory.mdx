import withPost from '../../components/blog/with-post'
import Image from '../../components/image'
import { meta } from '../../blog/webpack-memory-preview.mdx'
import Caption from '../../components/text/caption'

export default withPost({ ...meta })

Recently [Next.js 8](/blog/next-8) was introduced. The release included a massive build-time memory usage reduction. This blog post will explore how we have helped optimize webpack for the community.

Next.js is zero-configuration and is built on top of tools like [webpack](https://webpack.js.org/) and [Babel](https://babeljs.io). Its goal is to help you focus on whatâ€™s important: your application code.

Modern web applications consist of one or more pages. For example, a homepage, blog, dashboard, or product listing.

With Next.js, these pages become files in a special `pages` directory in the root of your project.

For example: the file `pages/about.js` maps to the URL `/about`.

One of the key design constraints of the framework is that it has to work well for both a single page and thousands of pages.

While implementing [Serverless Next.js](/blog/next-8#serverless-nextjs) it quickly became apparent that running `next build` on a project with hundreds of pages caused high memory usage. Sometimes exceeding the approximately 1.4 GB memory heap limit that Node.js has.

We started profiling memory usage of the build process using the Chrome developer tools.

In the resulting profiles we discovered a point at which webpack would allocate a chunk of **548 MB** memory all at once.

The amount of memory allocated directly correlated to the amount of pages, meaning more pages resulted in more memory usage.

<Image
  caption="The Chrome Developer Tools memory profiler showed 548 MB being allocated at once"
  captionSpacing={30}
  src="/static/blog/webpack-memory/allocation-before.png"
  width={2030 / 2.5}
  height={600 / 2.5}
/>

By going through the memory profile's stacktrace we were able to track down the function that caused the memory allocation spike.

The allocation itself came from [`source.source()` method being called](https://github.com/webpack/webpack/blob/v4.28.4/lib/Compiler.js#L334) which generates the resulting file and stores it into memory.

However by looking further up the function that calls the `source()` method you can see that [`compilation.assets`](https://github.com/webpack/webpack/blob/v4.28.4/lib/Compiler.js#L316) was being iterated over using `asyncLib.forEach`. Meaning that the [provided function](https://github.com/webpack/webpack/blob/v4.28.4/lib/Compiler.js#L317) would be called for every file in the `compilation.assets` array at the same time.

So this meant that if there are for example 100 pages, and each page has to be written to disk, above code would try to write all 100 at the same time, including generating all 100 files at the same time.

The solution for this issue is using a [semaphore](<https://en.wikipedia.org/wiki/Semaphore_(programming)>) to limit the amount of concurrent writes. Generally we use [async-sema](https://github.com/zeit/async-sema) for this, but in this case webpack already had a suitable method available on [neo-async](https://github.com/webpack/webpack/blob/v4.28.4/lib/Compiler.js#L8):

```js
asyncLib.forEach(compilation.assets, (source, file, callback) => {
  // etc
})
```

<Caption>
  Previous code that ran the function concurrently for all assets
</Caption>

```js
asyncLib.forEachLimit(compilation.assets, 15, (source, file, callback) => {
  // etc
})
```

<Caption>
  New code that runs the function concurrently for a maximum of 15 at a time
</Caption>

After implementing this concurrency limit and profiling the build memory usage again. We could see the memory allocation being split into smaller pieces of **34 MB**.

<Image
  caption="The profiler now showed chunks of 34 MB being allocated over time"
  captionSpacing={30}
  src="/static/blog/webpack-memory/after-limiting.png"
  width={2060 / 2.5}
  height={886 / 2.5}
/>

This change showed very promising results, however in practice the build still ran out of memory, so we kept profiling and investigating the issue.

By further inspecting the memory profile we noticed how after the [`source.source()` method was called](https://github.com/webpack/webpack/blob/v4.28.4/lib/Compiler.js#L334) the memory did not get cleaned up afterwards (garbage collected).

In webpack assets are generally instances of [Source classes](https://github.com/webpack/webpack-sources). These classes all implement a `source()` method that will generate the file source.

The profile showed that many assets were instances of [`CachedSource`](https://github.com/webpack/webpack-sources#cachedsource). The way `CachedSource` works is that when `source()` is called the result is cached in-memory until the asset is disposed.

Inspecting the webpack plugins Next.js uses showed that we had no plugins that called `source()` after webpack had written the file, meaning that caching the written value had no benefit.

After [collaborating](https://github.com/webpack/webpack/pull/8609) with [Tobias Koppers](https://twitter.com/wSokra) he has [implemented a new option called `output.futureEmitAssets`](https://github.com/webpack/webpack/pull/8642) which allows opting-in to the new asset writing behavior.

With this new behavior the chunks being allocated were reduced to **_182 KB_** over time.

<Image
  caption="After all optimizations the profiler shows chunks of 184 KB being allocated over time"
  captionSpacing={30}
  src="/static/blog/webpack-memory/allocation-after.png"
  width={1280 / 2}
  height={212 / 2}
/>

[Next.js 8](/blog/next-8) already has all these optimizations built-in. There is no need to change anything when using Next.js.

This optimization was introduced on webpack, meaning not just Next.js users, but all webpack users will benefit from these optimizations.

We will actively continue to improve Next.js and webpack memory usage and performance.
